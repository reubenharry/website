---
title: "In defense of sphexishness"
date: 2018-07-02T15:57:35+02:00
draft: true
---

I had the luck to overlap at Stanford with Martin Kay, a computational linguist who practically predates the field, with whom I shared an interest in philosophy, particular Quine. He is very much of the GOFAI (good old fashioned artificial intelligence) camp, and this post is adapted from something I wrote him in defense of more recently fangled methods.

review letter to martin:
			change and add:
				this runs along the lines of Searle's Chinese room, and
		call:
			in defense of sphexishness:
				the sphex as a criticism of new fangled AI
					other sphex examples:
						leopard couch
						machine translation

Imagine the scene: Echo is going about her daily

EXAMPLE



Echo: mechanical translation

**In defense of sphexishness**






	what i want to say:

		knowledge is

		directly responding to his worry: how can you know about world via text?

			Suppose that the world is just a model

				This, incidentally, is the sense in which I meant that Quine was involved FIX
					Rather than seeing the world as what is there, we see it as the model we impose in order to make sense of sense data.
						objects are efficacious myth


A second reply:


My immediate impression on reading your response to my comments was that we are largely in agreement: knowledge (including world knowledge) is often (perhaps always) implicit and flexible in a way which defies easy characterization.

	Your feeling is that for this reason, it could not be learnt from text.

	What could it be learnt from?


GOOD: Here is the line that I want to push: neural nets are of theoretical interest precisely because they are both flexible and systematic.

	This was what I was trying to show

One point I tried to make, albeit unclearly, was that the extent of what can be learnt from
	ml methods
		is surprising. And moreover, is surprisingly flexible

I wanted to follow up on the area in which I still disagree. Specifically, my position is that this flexibility and productivity of knowledge is actually the reason why I'm in favour of modern approaches to NLP. Admittedly, this may appear counterintuitive, since it is the reason you are opposed to them.

Broadly, the motivation is this: neural networks offer a way of distributing information which strikes me as holistic in all the right ways.

The reason I previously brought up the example of the task of ``neural style change'', i.e. the change of a picture from one

First to clarify what I mean by the productivity of knowledge: steeplechase

	Dan Dennett has a nice example. He imagines that scientists have developed a way to perform ``neurosurgery'', in which the brain can be operated on in a way which can remove certain pieces of knowledge. Jim undergoes neurosurgery to remove the

		This makes clear the sense in which world knowledge is holistic, and the relation of this property to
			systematicity

		holism

reply:

	so knowledge isn't rule based

		a comparable problem:
			quasiregular translation of sound

	when you translate a book, that is a translation of a part of the whole: in this way, all abstracts are antiholistic

% 	Q to explore: Is the difference between syntactic and pragmatic translation the same as the difference between modular and nonmodular translation: note that a pragmatic translation emphatically is not modularizable!!! GOOD


First off, I found this a really enjoyable read which summed up, I thought, the serious arguments against modern machine translation methods.

A crude summary of your viewpoint, as I understand it, is this: that modern NLP machine translation will not succeed in the long run, because it simply cannot gain the requisite world knowledge to perform real translation. You give a range of examples where it is clear that translation requires knowledge of the world, particularly in cases where the appropriate translation renders the sense, but not the letter of the source sentence.

% For example, you give the case of ``Assurez-vous que vous n'avez rien oublie' dans le train''. When deciding how to translate this, a pragmatic translator must decide what it is doing, and find something in the target language that does that.

One example you consider is the Winograd Schema in (\ref{bags}) and (\ref{notbags}):

\begin{exe}
\ex The city councilmen refused the demonstrators a permit because they feared violence. \label{bags}
\ex The city councilmen refused the demonstrators a permit because they advocated violence. \label{notbags}
\end{exe}

The point here is that the referent of ``they'' changes in likelihood from the city councilmen to the demonstrators depending on the verb in question. The claim is that a statistical machine learning system could not successfully identify the reference, because it doesn't have world knowledge of who is likely to fear and who to advocate violence. Moreover, it certainly needs to be able to identify the referent, since a translation could be going into a target language which marks pronoun gender, where the correct gender marking might therefore depend on who the referent is. This is a simple illustration of the fact that ``Nontrivial tasks, like translation, are AI-complete. In other words, any machine that can perform them would have to be able to mimic all aspects of human intelligence.'', which includes, of course, knowledge of the real world.

	% 	I think it's fair to say, therefore, that any defense of machine learning must show that a machine trained on input-output pairs \emph{can}
	% 	identify the referent of ``they''.
	% whatever constitutes world-knowledge for such a system can in fact successfully translate a sentence with this sort of ambiguity.



	% ``If one of these sentences is to be translated into a language in which the pronoun is required to manifest some kind of agreement, say in gender, with its antecedent, and if the two potential referents differ in this property, then the translator must solve the riddle.''


Implicit in this argument is a dichotomy between what you term syntactic and pragmatic translation. The former is what machine learning systems can perform well, while the latter requires the translator to understand the intent or meaning of the original utterance, and then render this in a new context, i.e. in a new language, or style. You argue that ``systems that are trained using these data will operate in a strictly syntactic manner, as we have characterized it.''

% ``the assumption is that the greater the number of training sentences considered, the better the estimate of these probabilities will become. But this is clearly true only if the overwhelming preponderance of the target sentences are the result of syntactic translation. Every pragmatically translated sentence is a red herring and the more of them that there are in the training data, the more the training process will be led astray.''.

The root of my objection to your perspective is with this claim, and more generally the dichotomy between these two sorts of translation. In particular, I'd like to argue that there is no clear distinction between these syntactic and pragmatic translations, and that all translation could be thought of as pragmatic to some degree.

	% , and perhaps more radically, that pragmatic translation is precisely what machine learning is good at and GOFAI (``good old fashioned AI'') is bad at.

Before attempting that, I noticed a few parallels between your views on translation and a number of related ideas in cognitive science, which seem worth discussing, by way of an introduction to where I'm coming from.

\subsection{Sphexishness}

Dennett, Hofstadter and others discuss the notion of ``sphexish'' behaviour, a term coined in reference to the observed\footnote{Although subsequently the biological facts have turned out to be more nuanced, the original example is still useful.} ``algorithm'' of the Sphex wasp when preparing food for its young. In short, ``the wasp's routine is to bring the paralyzed cricket to the burrow, leave it on the threshold, go inside to see that all is well, emerge, and then drag the cricket in. If the cricket is moved a few inches away while the wasp is inside making her preliminary inspection, the wasp, on emerging from the burrow, will bring the cricket back to the threshold, but not inside, and will then repeat the preparatory procedure of entering the burrow to see that everything is all right.'' -(\emph{Godel, Escher, Bach}).

The point is that the wasp's algorithm works just fine in normal circumstances, but needlessly repeats the burrow-checking step when one element of its routine is altered. The wasp is not able to analyze and modify its algorithm at all. It seems that this algorithm displays something unlike \emph{real} intelligence - a simulation of sorts which only resembles intelligent behaviour up to a point. The fact that the algorithm never changes, even when it would be useful for it to do so, shows that the wasp doesn't really ``understand'' what it is doing.

Sphexish behaviour seems like an apt analogue to the notion of syntactic translation, which lacks the flexibility of pragmatic translation. To draw this connection out more clearly, take the example you provide of:

\begin{exe}
\ex	Assurez-vous que vous n'avez rien oublie' dans le train. \label{base}
\ex	Make sure that you have not forgotten anything on the train. \label{syn}
\ex	Make sure that you take all your belongings with you. \label{prag}
\end{exe}

A syntactic translator, as you envision it, might be able to perform convincing translations in simple cases, but as soon as there is a situation where world knowledge is required, its shallowness is exposed, just like that of the Sphex wasp. One could say therefore that the dichotomy between sphexish and real intelligence mirrors that between syntactic and pragmatic translation. So one could phrase your objection to modern machine translation (MT) as being that it is fundamentally Sphexish, as a result of the way it learns.

% 	the point: flexibility: in what dimension?
% 		note: there is also a notion of context of utterance. relevant?
% A syntactic translation like (\ref{syn}) seems adequate up to a point, but alter the context and it becomes clear how short it falls. For example, if
% 			the context here equals the input sentence and the algorithm is the translation process


% It seems that, just as with the Sphex wasp, one could not expect any more flexibility unless real understanding were at play. Thus the syntactic translation process is comparable to the Sphex's ``algorithm''.


% Another famous Gedankenexperiment , the Chinese room, seeks to demonstrate the non-equivalence of Sphexish behaviour to real intelligence by

In summary, the notion of a dichotomy between processes that require real understanding and processes that simulate real understanding is broader than translation, at least in the narrow sense. In this spirit of this sort of generalization, I feel it's worth mentioning Douglas Hofstadter's perspective on translation.

\section{Hofstadter's Perspective}

Hofstadter has a very broad notion of what it is to translate. For one thing, it extends beyond language in the conventional sense to cognition and  perception in general.

Speech recognition, for example, requires us to abstract the acoustic signal into phones, then phonemes and later words, all abstract units. To Hofstadter's mind, we are making an analogy when we identify a symbol like a phone, namely an analogy between different sound patterns in the acoustic signal. Two instances of a \[p\], for instance, might be very different as regards their constituent sound waves, even if we effortlessly treat them as the same. The same applies to larger units, like words.

The general idea is that abstraction is about understanding what can change on the low level (e.g. the acoustic signal) while the high level (e.g. the phoneme) stays the same. Translation in the literal sense is about this problem too: when translating a book from French to English, every character is likely to change, but the general ``meaning'' of the book stays fixed. In a very broad sense, then, one could say that translation is the act of preserving something, e.g. content, across a change of form.

% Vision, for example, requires us to break the world into objects, like people.

In terms of cognition, rather than just perception, Hofstadter highlights numerous cases in which humans are easily capable to perform all sorts of abstract translations, or analogies. He gives examples like the ability to answer the question ``Who was the Gandhi of South Africa?''. This is a translation in the sense that we are preserving the role that Gandhi performed in India into a different context.
 % who was the first lady of Britain when Thatcher was prime minister?

One further example is the translation of artwork from one style into another. As with the other cases, this is a pragmatic translation in the sense you defined it: to copy a Turner painting into the style of van Gogh, for instance, we need to recognize the objects in the painting, and understand the constraints of each style, rather than just see it as a collection of pixels. So presumably the same argument against machine learning attempts at this task should apply.

% and an analogy between the shadow
	% Metaphor:

	% Suppose we think of blue as having a syntactic meaning, to do with a wavelength of light, and a pragmatic meaning, which allows extension to blue cheese, blue days, blue moods, Picasso's blue period, and so on.

	% We can think of this as a sort of translation. On the one hand, we have syntactic translation, which preserves ``blue'' in ``The sky is blue.'' and ``The ocean is blue.''. On the other hand,

	% 	You're changing the object, but preserving this one attribute.


% One could see this as comparable to a linguistic translation, where the image of the painting is preserved, and the parallels to the two languages are the two styles.

% The link to your stance on translation is that this broader sense of translation appears to be AI-complete in just the same way as literal translation. In order to make a cubist version of Van Gogh's sunflowers, one has to have an understanding of what the image is representing, rather than just see it as a collection of pixels.
% 	to take his example...imagine two situations where we use the expression ``That's the pot calling the kettle black if I ever saw it!''


% 	not clear: later part focusing on the ACT of translation is clearer
% 		Douglas Hofstadter's views on cognition put forward a very broad notion of translation, which he terms analogy. This encompasses any perceptual judgments of sameness. For example, we make an analogy between a wide variety of acoustic signals all of which are some instantiation of the letter ``p''. Or a much more abstract example: we make an analogy between the roles of the prime minister of Britain and president of America when saying that they have the same job.
% Hofstadter's point is that the human ability to make such fluid analogies, which alter form but preserve content, is the key to cognition. Without getting into a discussion of whether this view of cognition is too fanciful, we can simply note that analogy offers up many comparable cases to traditional translation, which seem to offer a similar problem.





One other key idea of Hofstadter's is that real understanding is obtained through a hierarchy of levels. For example, take the task of translating an image from one style to another. We could not to this by simply mapping pixels to pixels. Instead, we first abstract the image into a collection of shapes, then assemble these shapes into larger shapes, and then scenes, such as a person walking down a street. This gives us an abstract representation of the picture, which we can then map into a different low level form.

This hierarchy seems present in linguistic translation too. It would be impossible to translation on the basis of phonological form alone. Instead, we abstract a string of characters into words, these into syntactic and then semantic structures. The hope is that the semantic structure, in the broadest sense, remains invariant under translation. So we could think of the process of mapping in and out of this abstract representation as tantamount to translation\footnote{I suspect this is the point where our opinions diverge, as I'm presenting semantic form as a sort of abstract representation of words, but haven't said much about its relation to the world.}.

Related to this point, it is worth noting one key property of translation: it is not a structure preserving map. In other words, the constituents of a sentence in one language do not correspond to constituents in another. Likewise, the translation of a book from one language to another does not preserve each sentence. So we can't do something straightforwardly modular when performing translation, like breaking it down into the translation of constituent parts.


	% The task of semantic ``compression'' (i.e. summary) for example, suffers from similar difficulties. It too is a common task for modern NLP techniques, and again, the same argument seems to apply: how can a machine trained only on language manage to
	% 	perception

% Suppose, for example, that we want a machine to recognize a word, say ``hello''. The acoustic signal behind this word could (and does) vary wildly between speakers and utterances.

% equivalence class



	% ``If analogy were merely a special variety of something that in itself lies way out on the peripheries, then it would be but an itty-bitty blip in the broad blue sky of cognition. To me, however, analogy is anything but a bitty blip — rather, it's the very blue that fills the whole sky of cognition — analogy is everything, or very nearly so, in my view.''


% It seems that a system which is capable of making or judging such analogies must be able to understand what is meant



% \section{Mcclelland}

% Thirdly, a debate which prefigures the modern clash between machine learning and symbolic systems is the connectionism vs BLAH




% Vastly more examples come to mind of parallels between the dichotomy of syntactic and pragmatic translation, and BLAh, but these illustrate BLAH well.

% Quasireg: idioms


% The point, it seems to me, is that this dichotomy plays out in all sorts of instances.

\section{Defending Modern Machine Translation}

I'll now try to say something about where I stand as concerns the dichotomy between syntactic and pragmatic translation. As I said before, my aim is to show that this dichotomy is really a continuum, and that what modern NLP techniques achieve is not fundamentally different in type, but just degree, from real, pragmatic translation.



	% Take the translation of ``'': sure enough, we parse it syntactically, and then match piece to piece. But what is syntax?
	% 	No, this is too hard to show.
	% that's X for you: the sarcasm isn't part of the language: how would we translate: that's Jim for you: what would a syntactic translation mean? word by word? sound by sound?
	% 	the nonexistence of syntactic translation is crucial but hard to show and hard to explain the point of:
	% Even the most procedural of syntactic translations you provide is pragmatic to some degree
	% a hierarchy of levels of abstraction in translation

	% his definition of syntactic translation is not clear: it doesn't exist...
	% ``Assurez-vous que vous n'avez rien oublie' dans le train which can be translated syntactically into English as Make sure that you have not forgotten anything on the train.''
	% 	in what sense is this syntactic? it isn't purely lexical. nor is it purely syntactic...

	% 		why?
	% 			well the assumption is there's a syntactic structure, and we can translate via this:
	% 				it isn't clear what evidence we have for this
	% 					does it always yield a translation?



% ``The inescapable conclusions are that statistically-based systems cannot extract information that is not explicit in the source text because it simply is not there to be extracted from data that has no referential component, and it will often treat explicit information incorrectly because it is precisely the divergence of a text from what is expected that makes it worth writing in the first place.''


\subsection{The empirical objection}


	% So, for a Winograd schema, resolution requires world knowledge.

So far the discussion has centered on what machine learning ought to be able to do. To gain a sense of what machine learning \emph{can} do, consider the following three images, from \url{https://deepart.io/}:

\includegraphics[scale=0.2]{vangogh1}
\includegraphics[scale=0.2]{vangogh2}
\includegraphics[scale=0.2]{vangogh3}


The first is a photograph. The second is a painting by van Gogh. The third is a ``translation'' of the first into the style of second. Looking at these, it seems that one would have to have knowledge about the world, e.g. what a house is, as well as understanding van Gogh's style, in order to perform this translation. And yet, this was done entirely by a neural net, which has been trained on a large set of images of the world.

Consider the way the neural net has succeeded in preserving objects in the photograph, trees, building, etc, but has converted them into van Gogh's style. The mapping does not preserve every object, and indeed adds some, such as stars, but it seems clear that it has done precisely the sort of thing which it seems it should not be able to do. This strikes me as a good example of how the sort of world knowledge that is needed for translation is not fundamentally out of a neural net's reach.

If nothing else, it suggests that ``mere'' syntactic translation goes far beyond what we imagine to be a clumsy, inflexible process. Instead, neural nets are able to do something which looks rather like the hierarchical process of abstraction envisioned by Hofstadter.

% This is again the distinction between syntactic and pragmatic translation. The claim is that machines might perform well at the former, but never the latter. The image translation task, one might argue, is a syntactic translation, i.e. a sphexish one.


I'd anticipate several responses to this claim. One response might be to say that this image analogy problem is not comparable to the problem of translation in language. But they certainly \emph{seem} similar. They both take a structured piece of data which requires world knowledge to be interpreted, and perform a translation which preserves the content while changing the form.

Relatedly, one might argue that the task the \emph{deepart} translation succeeds at is fundamentally unlike pragmatic translation, being perceptual rather than cognitive. Indeed, this seems roughly to be Ron Kaplan's position; he acknowledges the success of voice recognition systems which use machine learning, but draws a distinction between perceptual and cognitive faculties. But speech recognition appears to require world knowledge, as does the art style change problem, so that the argument against modern statistical approaches ought to apply here too. The fact that the facts say otherwise seems, therefore, to require some explanation.

One final response would be that the translation evidenced in the above paintings is still sphexish in some way. But if this is sphexish, then what isn't?
	% then what on earth is not? Surely this translation demonstrates exactly the sort of understanding that humans appear to have.




% The deepart example is only one of many that machine learning has had success with. Others that are similar include:

% 	\begin{itemize}
% 	\item performing: the translation from an image in one style to an image in another
% 	\item performing a translation from sound to graphmes
% 	\item performing a translation from perception to action
% 	\end{itemize}



\subsection{The philosophical argument}

So far, this isn't really a philosophical argument, so much as an empirical one. So what can we say by way of explanation for why deep learning has so much success at tasks which appear to require world knowledge?




	 % Well, the idea is that almost everything we conceive of as translation fits the bill of \emph{pragmatic} translation.
The parallel between syntactic translation and sphexishness provides an initial clue. Philosophers such as Daniel Dennett have spent quite a lot of time trying to show that the dichotomy between sphexish and truly intelligent behaviour is misguided.

Recalling the notion that abstraction is a hierarchical process, it seems that there can be different degrees of sphexishness. For example, statistical image recognition has been known to mistake a leopard skin couch for a leopard (c.f. \url{http://rocknrollnerd.github.io/ml/2015/05/27/leopard-sofa.html}). The reason is that its method of recognizing a leopard is sphexish: it just looks for the skin pattern, not the 3D shape. Still, it could have been even more sphexish. It could have just looked for orange pixels next to black ones, or some similarly low-level approach.

So the question is whether its mistake in identification of the leopard skin couch indicates a fundamental problem, or just that the neural net needs more layers in order to abstract a 3D model of a leopard. If we take the latter view, then we are effectively saying that even human intelligence is sphexish to some degree, only at a much higher level. This is roughly where I stand.

% It's another question, though, whether the ability to


But even if machine learning is good at dealing with hierarchical structure in a suitably systematic way, it isn't clear that it can have world knowledge. A second argument in defense of machine learning, therefore, would be to press on exactly what it means to have knowledge. It's very hard - almost suspiciously hard - to say exactly what knowledge of the world constitutes. This seems at first like a quibble at best; it may be hard to pin down world knowledge, but clearly we have it.
% I think we have to tread extremely carefully though.

To really hone in on this issue, take the example of the Winograd schema, with examples (\ref{bags}) and (\ref{notbags}). The knowledge required to identify the referent of the pronoun is: who is more likely to fear and who more likely to advocate violence.


Of course, if we were trying to encode this knowledge into a machine, we couldn't just key it in, so to speak. That is to say, we couldn't just add a proposition that the probability of a demonstrator being violent is more than that of a city councilman.
The knowledge is rather flexible, and it's certainly not trivial to think of a proposition which captures it. Part of the reason for this is that it is difficult to separate this knowledge from knowledge regarding the rest of the world\footnote{Dennett gives the thought experiment of removing one piece of knowledge from the mind. What else would also have to go?}. Furthermore, there might be contexts in which the proposition that demonstrators are more likely to be violent isn't true.

One could see this as a sort of Quinean point: beliefs and knowledge are all interrelated to the extent that any attempt to separate them into separate clearcut propositions is doomed.


As an example, Hofstadter considers the meaning of a note in a symphony. It would be hard to say what the effect of the note was on the music separate from its context in the music itself. Instead, we can think of the note's meaning as the effect it has within the context it appears. Compare this to a neural net which is translating a sentence. It is hard to say that the effect of each word on the meaning of the sentence is exactly, but with the analogy of the note in mind, this does not seem overly problematic. In other words, the distributed nature of representations in a neural net could be seen as a feature, not a bug.

Another example of knowledge being distributed is the case of tacit knowledge. An example of tacit knowledge is the ability to ride a bike. It would be hard to put this knowledge into words or even into a set of rules. Or, to give a linguistic example, note that it's hard to define what a sandwich is. But translating the sentence ``John ate the sandwich.''  into a language without a word for sandwich requires us to find a word in that language denoting an analogous concept. Or at least a sentence which conveys the same ``proposition''.

Either way, it seems clear that we have to know what a sandwich is. If we conceive of this knowledge as something denoted by a set of rules, then traditional approaches to translation seem promising. But if we instead think of the knowledge as being operational, i.e. as the ability to use the word and concept ``sandwich'', then a more distributed representation of it seems appropriate.

		% it seems that what a sandwich is doesn't boil down to a single rule. you could almost say that the knowledge of how to translate it is the definition:
		% 	impose largening, and you're doesn
% Thus we have a philosophical problem, the difficulty of representing pieces of knowledge separately, and a perceived drawback of a neural nets, the inability to represent pieces of knowledge separately. Putting these two together, I'd argue that the lack of explicit rules in a neural net isn't a bug, it's a feature.


% This sort of problem, sandwich burrito, seems to be why translation is not a structure preserving map.
% 	seems like you should say something about the fact that the parts of a sentence don't map
% 		and in the upwards direction, this loses you structure preserving map:
% 			note, this is where distribution fits in:
% 	And as you increase perspective to the next largest unit (sentence, paragraph, chapter, and so on), the same phenomenon seems to repeat.
% 	This holistic property of translation seems to me to be exactly why a modern statistical approach is beneficial.

% I would go even further, and say that a neural net is well adapted to dealing with interconnected information. I think proving this intuition would be difficult though.


% My view is that this sort of knowledge is something which a neural net can acquire from training data. More generally, all knowledge is acquired through a similar process of training



% Rather than try to say why this \emph{is} true, I think it's more convincing to show why the intuitive objection to it falls flat. In particular, the objection is that training a neural net will never lead to it having world knowledge, because world knowledge is systematic and structured.

% 			The response to this objection is that world knowledge is neither of these things, and can perfectly well be acquired by a neural net. In fact, this is the only way it can be acquired.

% the indeterminacy of translation first
% 	note that translation is not a structure preserving map.:
% 		but we can reside in a category in which it seems like a structure preserving map.
% 			there's more: self morphisms aren't...but we can create a kleisi category in which they exist


% In other words, what actually is a meaning? To put this into more concrete terms, take the Winograd schema

% So my claim is that whatever nebulous object meaning is, it \emph{is} this which blah captures

% 		What about knowledge that is harder to put in such a form? For example, people have a sense that goldfish are more typical pets
% 			a productive example: surely this isn't stored somewhere...




% Let's take a much more subtle case of knowledge next.
	% ``Say what you will, but oreos are the best cookies out there.'': is syntactic translation even well defined in cases where...



\end{document}

\section{Distributed Representations}

While neural nets do not deal in symbolic information, manipulated in a systematic way, it isn't the case that they cannot deal with structure at all. For instance, so-called ``deep'' nets (i.e. multi-layered ones) are used for image recognition tasks because they can hierarchically build up

These symbolic units, e.g. lines, objects, are never explicitly represented
	but can be thought of as distributed representations


long thin pushbacks. the book changing parabel: function on letters to change the meaning: sphexing = this: show. note: this is what body does
	you believe the neural net is emulating a real thing we do, eg eye recognition, but the twist is that the real thing we do is also a stage

``It is in contrast with what we can refer to
as pragmatic tradition, in which the translator adopts a position similar to
that of the original author and, having understood what that author is trying
to say, seeks a way of saying it in the target language, without attempting to
relate individual words and phrases in one language with words and phrases
in the other.''


Quine's idea: no translation, or: translation is holistic: objects exist only within the system.
	Why distributed representations are fundamentally Quinean:
		distributed representations are how you do translation in a context including way...


	marr's fallacy of translation: levels are separate: different levels are independent: you can translate between implementations

	points: symbolic view assumes translation:
		take case of visual recognition system: dogs, we argue, are quasiregular, but a symbolic system has to have a fixed notion which it can pass on

		more generally: symbols are like thinking a book has information in it: they are natural kinds...quine's view is holistic: frege's context principle...


		POINTER
			The two-route system cannot capture quasiregularity.

note: three quinean points:
		translation same as abstraction of objects: word vs object
		no natural kinds

I suppose it comes down to whether the problem of hierarchy is the same as that of world knowledge.


ok, stuff:

	ok, so start working on your metaphor project: you can see possible empirical stuff measurable.

	stumped on abstract nouns: obvious feature selection approaches, not so clever though

	he wasn't aware of the quinean connection: i think this is my own insight: should test on m kay

	his view of cognition is still not fully distributed in the way i'd advocate. no making of the final join

	i am truly terrible at explaining myself, when it comes to vagueness: the point about pedantry and vagueness just makes no sense at all out of context:

	idioms: my view is that idioms are just processing strategies, and if you look at really abstract ones, they're hardly even real

	he understood my analogy between the two streams for writing, and the two streams for metaphor

	graphemes: are they as symbolic as they seem? I dispute it...consider the influence of font: can we separate this out?

1. applying his ideas to semantics: propositions as internal to a system: pdp as a realization of quine...
2. How general do you think quasiregularity is? e.g. any cognitive symbolic system? translation, perception, hofstadter's view of analogy
	GOOD: i'm interested in doing formal work to capture the notion of quasiregularity
	 	capturing connectionsist equivalent of compositionality with monad
	 	type based extraction
3. is there any stopping point?
		e.g. for grapheme to phoneme: would it be even better if it were grapheme to meaning? or grapheme to action? (practical concerns aside)
4. Why is metaphor relevant to quasiregularity? AHA: if concepts are quasiregular, then the literal/metaphorical dichot collapses:
	i.e. something like gold is already distributed. then metaphorical extensions are really of the same type.

GOOD:		see: blue cheese: see intensional adjectives: the paper on them: red chair: AH: red isn't doing something regular to chair...

		GOOD: see fake gun: is a type of gun because gun is quasiregular: i.e. not natural
5. idioms: do you know of any work that's been done on idioms


``First, a fundamental motivation for avoiding a fixed taxonomy of units is the existence of continuous variation
in the characteristics of the purported building-blocks of language.''

``It could be argued that love has many different meanings, each of which can
be listed separately in the lexicon, but where do we draw the line? I would argue that
even in the case of John loves Mary and Mary loves John, the meaning of love is slightly
different, and that, in general, the meanings of words are not selected from a fixed taxonomy
of alternatives, but take on different shades in different contexts that cannot be
captured by a fixed taxonomy.''

to focus on: what quasiregularity is, and how it applies to semantics:
	all the different notions of ``run''

for the grapheme to phoneme thing: I see graphemes and phonemes as internal representations, in a larger process:

So my main question is how you think of quasiregular objects: are they internal representations to a neural net?
	I don't think he does. he thinks of them as outputs

the emergence of natural kinds: feeding info into net vs having it emerge: about natural kinds
pdp is holistic


take a task like word stream to meaning calculation. it seems to me that in the long run, you'd get even better results if you got rid of even more structure, so the network could be freer. so input acoustic signal.


Example of internal representations as quasiregular:

	why would I think this anyway?

		what's the alternative? output is like the irregular verbs in English

think about sorites for distrib systems

i'm interested in his view of compositionality: i see compositional semantics as a form of GOFAI








POINTER:
	example of join (see appropriate file): formalizing this property: seems crucial: does mcclelland have an example of join in his paper?



\section{}
\begin{itemize}
\item The relation of vagueness to quasiregularity
\item a computational perspective on denotations: denotations as functions from world to categories: but subsymbolic!!
\item Function subsumption:

\end{itemize}

\section{intro}

I'm interested in talking to him because he has a perspective that doesn't just use connectionism for cognition, but argues that it's appropriate, in particular with regards to quasiregular features of language.

	ah, coming to it from the other direction: the two pieces of the puzzle: there are philosophical reasons to be suspicious of symbolic representations

i see formal semantics as GOFAI: symbolic representations of meaning

does he agree that: distributed representations aren't just vectors, they're embedded in the system, in a fully holistic manner.

\section{Philosophy}



	phonasthemes:

How extractable do you think distributed reps are? I think not at all...

	I think more stuff is distrib reps than we usually think: often we think thing is an output, but it's medial.

GOOD:
	we can take a system and show that it's part of a bigger system and it's internal:
		GIVE one of his examples



			a behaviourist perspective

\section{My Project Ideas}

AH: non-natural kind vagueness: i.e. los angeles: no underlying precise area: perfect as a distributed representations:
	dealing with issues like sorites and fuzziness in a system where the objects aren't really there

Why is metaphor relevant to quasiregularity? AHA: if concepts are quasiregular, then the literal/metaphorical dichot collapses:
	i.e. something like gold is already distributed. then metaphorical extensions are really of the same type.

		see: blue cheese

		GOOD: see fake gun: is a type of gun because gun is quasiregular: i.e. not natural

	GOOD: what gold is: your notion of metaphor lines up very strongly with macclellands quasiregular view of natural kinds: i think this is what's at stake in the problem of metaphor
	 ``Natural kinds While this domain might be excluded from language by some, for those who see language as exemplifying domain-general principles, not to mention reflecting the structure of the natural world, this is an important domain to consider alongside of more properly linguistic domains. It might even be argued that cognitive mechanisms that evolved in pre-linguistic hominids evolved to be useful for capturing the quasi-regular structure of the natural world. This domain clearly exhibits quasi-regularity, in the sense that many items are partially but not totally consistent with the typical features of their taxonomic category. Elephants and turkeys are good examples. Elephants have many of the typical properties of mammals, so it is clearly useful to see them as members of this class, but they also have several idiosyncratic properties. Their large floppy ears and trunks are unique, while they share having tusks with a few other animals. Turkeys share many properties of birds, but are members of a sub-regular cluster of flightless birds (though flightlessness itself a matter of degree – wild turkeys can get off the ground for short distances), and they tend to share with such birds their superior edibility compared to many birds that fly. Clearly, then, the domain of natural kinds exhibits both quasi-regularity and sub-regularities.''

for translation: spell out point that analogy is quasiregular: it has systematicity but isn't symbolic: e.g. mutatis mutandis:
	think about how abstract objects are equivalence classes under translation



\end{document}

key nonobvs ideas in pitch:
Section 1: Hofstadter:
		text presi: Hofstadter's understanding is incredibly subtle, so this is by no means a simple summary. I also don't think it's redundant: I've adapted his views into my language, and highlighted certain parts more than others. The connection to Godel and logical incompleteness, for example, doesn't figure anywhere in what follows
	Abstraction is the mapping from object into a representation which is invariant under some translation: hence the translation we want guides the abstraction:
		e.g. sound, deepart, actual translation:
	Meaning is linguistic abstraction: it is no different: we want the invariant
			note: this is the connection between translation and perception and abstraction and meaning, in the cognitive sense
	The big question is how this abstraction works, or equivalently, how translation works.
	It appears hierarchical: examples
	A key question: what is in the object, and what is in the world, when performing abstraction: a key dichotomy: suspicious hard to solve: holism vs reductionism
	Other perspectives on abstraction: explanation of a system as translation. compression as translation. modality. others here: human intelligence is not natural kind: i.e. sphexishness and context sensitivity, algorithms as abstract: proofs as abstract: wlog
	Links to philosophy: natural kinds, belief, morality,
	Generalizing to natural kinds: i feel like a step is missing here
Section 1.5: further remarks on intuition:
	removability of a part: separability
	mut mut
	marr

Section 2: empirical and philosophical doubts over this notion:
	quining the dichotomy

\section{Quasiregularity of phonasthemes}

``n the lexicons of many of the world's languages, there seem to exist subword patterns of
sound and meaning that cannot easily be analyzed as morphemes. English, for example, has a
number of words that start with the consonant cluster gl- and share a meaning related to light or
vision, including glimmer, glisten, glitter, gleam, glow, and glint. Firth (1930) coined the term 1
PHONESTHEME to describe such patterns. ''

my view is that the mapping from sound to meaning is quasiregular, and that phonasthemes give an example as to why.


	wondered if I could talk to him about connectionist approaches to vagueness...

	metaphor as quasiregular because it's usually treated in a two stream approach, but actually if all is treated in a single stream...

	assumptions of quining compositoinality are shared! good use of the coinage here

	I'm a first-year PhD student here in the linguistics department, and was just reading your paper on "Capturing Gradience, Continuous Change, and Quasi-Regularity in Sound, Word, Phrase, and Meaning". I found the perspective particularly interesting, because of the analogy it made between quasiregularity in different domains (e.g. semantics, syntactic idioms, phonology). I'm interested in research in the area of connectionist approaches to semantics - in particular I've been thinking about the notion of vagueness and
		 and I was wondering if you had any time to meet over the next few weeks to briefly discuss some directions I could take.

	1. do you think quasiregularity is also regularity? e.g.
	2. phonasthemes

	Hi,


	I'm really interested in the philosophical issues to do with distributed representations, particularly with respect to what it means to extract
		and am keen to look at phenomena in language from a connectionist perspective.
		e.g.
		Distributed representations of phonasthemes
		Imprecision and vagueness:
		Metaphor: it's quasiregular in that there isn't a two stream route between metaphor and literal meaning.
	I had a couple of questions about your view on language and cognition along these lines:

	PSYCH 209

		Firstly, do you see the phenomenon of quasiregularity as a wider issue in understanding abstraction, in cognitive science? It seems that the whole problem of concept categorization

		I don't know how much you've come at this from a philosophical angle, but the idea that natural kinds are quasiregular seems to be a sort of cashing out of Quine's suspicion about natural kinds DID He?

		Firstly, a philosophical question. Distributed representations can "represent" symbolic knowledge, e.g. knowledge of categorical distinctions, but it seems that people often act as if this distributed knowledge can be extracted from the system. Surely the whole point is that these representations can't be extracted, since they only exist with respect to a task at hand.

	If you have some time in the next few weeks

	GOOD: mutatis mutandis and abstraction in maths


	quasiregularity is often mistaken for "systematicity".
	a: how much is quasiregular? i'm interested in things which seem regular but are quasiregular
	b: compositional semantics: a bad game






	For instance, translation seems like a


	I'm quite keen on doing research about quasiregularity in language, from a computational and/or philosophical perspective, so let me know if you think there are any interesting directions to be taken.

	``surface representations (i.e., pat- terns used as inputs or outputs of the distributed neural network) that were specified by the modeler, and quite often characteristics of these surface representations are themselves problematic, particularly in that they tend to be discrete and categorical in nature.''




	he finds quasiregularity in: natural kinds, morphology, and idioms.
		Note my examples:
			metaphor
			sameness
			vagueness

		Quine's suspicion with translation seems to be a


email to mike:

		putting together a speech recognition system with a question answering system:
		why modular? a speech recogition system isn't modular on the inside

			how to resolve ambiguity gloabally rather than locally

POINTER
idea: often you have cases where there is something we see as an output of a system, but it's actually internal in a larger system:
	first show internal representations can't be extracted
	then show that supposed endpoints are actually medial


	GOOD EXAMPLE: when you see your bike is stolen, no matter how you see it, you want the knowledge to be separate from the fact, so you want an explicit rep of the fact.
		of course, there's lots of stuff going on around this, like white noise (good analogy) so the job of the researcher is to peel the noise from the rest: quinian intuition is that there's no underlying thing.

	note: basic pitch idea: on one hand, composition. on the other, lexicon, which is brittle. but now semiproductivity.

	Q: why is GOFAI based on natural kind error?

		OK, another attempt at some basic ideas, this time a little longer:

			I've divided this into three parts. The first attempts to lay out what I think Hofstadter's worldview is, as conveyed in Godel, Escher, Bach, and other works. In doing this, I'll inevitably repeat his points in less elegant words, but I think it's important to lay a foundation for the perspective I want to develop.

			\subsection{Hofstadter}


			Hofstadter is obsessed with a single question in a variety of forms: how do humans perform abstraction?

			This revolves around what it means to abstract, of course. So a few examples are in order. Hofstadter offers these in endless supply,

				examples

			The conclusion is that abstraction is about preserving content while changing form.

				to note: this is a cognitive process. so epistemic.

					not concerned with what things are, but with what they look like. but these merge.

				note isomorphism of abstraction and translation, via equivalence classes

			We could think of the content as a type and the form as a value. Abstractions are then type judgments. I believe this is roughly Martin-Lof's view. It's worth noting, as it will be useful later.

			1: obvious premises: modularity, i.e. breaking up a hard problem into separate subproblems, has obvious benefits. It means that you can change parts of a system without changing the whole, e.g. modular code is translatable, in the sense that when you want to change something about what it does, you don't have to change everything.

			Relatedly, abstraction has obvious benefits. For example, if you have a system built to recognize faces, you probaly

				This flexibility is the goal of abstraction: you want a system where the high level is separate from the low level. What that means in this context is that

				similarly, high level code is translatable across different hardwares. In other words, there's a notion of the code that's separate from the implementation.

			However, biological solutions often tend to be "long thin solutions", i.e. very low level units have a direct interaction with high levels. The coding analogue would be code that was highly optimized for specific hardware.
			Again, this is an issue of translatability: long thin solutions are very context sensitive, so translating them to new contexts (i.e. changing the problem slightly) messes them up.

			Clearly humans are very good at translation in lots of domains. Phonemes are abstract in the sense that you can change a person's voice, their pitch, tone, speed, gender, etc, and the phonemes don't change. Vision does the same. A different example would be meaning: we want the meaning of an utterance, or even a book, say, to be invariant across translation (here I mean translation in the literal sense, from one language to another, or of length: translating a book to a shorter length is compression, which is common NLP task). So the intuition is that you preserve something across the translation in all of these cases. Knowledge is similar: if you can drive one car, you can probably drive another, even though the pedals, etc. are in slightly different places.

			2: so with this in mind, the big question in cognitive science is how we do translation, i.e. abstraction. Likewise for AI, the question is how to get computers to do it. The GOFAI assumption is that we work our way up through a hierarchy of increasing abstractions. E.g. for vision: lines, objects, all the way up to really abstract things like handwriting. So a high level symbol concept like the BLAH is built up of simpler abstractions, and so on until you get to something simple.

			To belabour the point, an abstraction is formed by translating across lots of instances. Symbolic units like phonemes, or objects in vision, or meanings in text, are abstract, because they only exist once we've filtered out ``noise''. They are effectively equivalence classes over translation.

			Note how this is basically the same as Plato's question: what makes a horse a horse, etc. The question is how abstract objects are related to the "low level" physical world. Plato's forgivable, but idiotic line, was that objects have the
				This is all about quasiregularity, when you think about it...

			To put this more clearly into the context of GOFAI, Marr, one of the pioneers of cognitive science, introduces the paradigm of levels of explanation. Three in particular. The idea was that a process could be viewed functionally as BLAH, then algorithmically, and then



			Each level is not isomorphic to the others. This lack of isomorphism is the linchpin of abstraction. You can get on with a thought without having to worry

				One could say that this is the whole desideratum of cognition, and the key to understanding it: a processing system which can ignore details

					in Borges' evocative line: ``To think is to forget a difference.''

			So the way in which level independence is achieved is the object of study in GOFAI. Take a minute to note that level independence is equivalent to a belief in the possibility of translation. How so? Well if you can have an algorithmic level remain the same while the implementation level changes, you have preserved something

			Other examples:
				modality: To say that Nixon could have lost the 1968 election is to translate to a (fictional) world ETC

			So far, so obvious. The problem, of course, is that

				If you think really hard about why this is, perhaps, just perhaps you will begin to wonder whether the goal is really achievable at all.

			the suspicion that Quine has (indeterminacy of translation) along with various other philosophers (Kant, Wittgestein, Dennett, Hume), is that translation isn't possible. Or rather, that there isn't really anything preserved across translation absolutely. Instead, with respect to a given context, something can be preserved. For instance, suppose you're translating The Iliad from Greek to English. It seems pretty obvious that there's a meaningful way of doing this, but what exactly it is depends on the goal of what you're doing. In other words, the translation is only defined in a larger context. The corollary of this is that abstract symbols don't exist independently of the system they're in.

			At first this seems like a somewhat banal qualm.

			This suspicion about translation is borne out by the difficulty of actually pinning down abstract units. For example, it seems obvious that the sound /b/ has some reality, but getting a computer to take a sound and decide whether it's a /b/ is of course very tricky. From the GOFAI perspective, this would be because it's just a very hard problem, but another perspective is that you can only decide whether a sound is a /b/ within the context of its use in a larger context.
			Of course, it SEEMS simple, precisely because humans do abstraction so naturally.


			4: What this has to do with natural kinds: a natural kind is something which is what it is independent of context. traditional examples would be things like water: kripke gives all these examples of how on another planet (twin earth), if you had a substance that was just like water except it had a different chemical composition, it wouldn't be water, because water is what it is in virtue of its chemical composition. So you can decide whether something is water without taking its surroundings into account. "mammal" might not be a natural kind, since its definition is manmade and depends on the goal of what you're trying to do with it.

			Another way of looking at it is that they are things which are what they are independently of how we determine what they are. They have an essential property which makes them what they are. This means that they are easy to identify
				(interestingly, this is a Kantian point: his version of the "no natural kinds" intuition is that we confuse phenomenal and noumenal: i.e. the category error)

			The intuitive generalization of natural kinds, therefore, is as translatable objects, following the broad sense of "translation" we've been using. In other words, they are things which are preserved across contexts.

			GOFAI: a horse is defined via a complex decision process
			Quine: No, ``horse'' isn't a meaningful category outside of a context of use.
			GOFAI: What on earth does that mean? Clearly we have the capacity to
			Quine: In which case,

			The reason that I think this is a useful concept is that there are all these debates in linguistics, philosophy and so on that are subtly the same, but happen over and over again. To list a few: the distinction between free will and determinism, the two-stream vs one-stream account of reading, the nature of explanation, GOFAI vs modern AI, the proper treatment of metaphor, the philosophy of consciousness (it strikes me that going through these in more detail and explaining what they have to do with natural kinds would be a good idea)

			A less intuitive abstraction is actually a visual object, e.g. a chair.


			A lot of things SEEM like natural kinds, but aren't on closer inspection. Phonemes are one example. Again, it's obvious why we'd think something was a natural kind when it wasn't: because our minds factor out noise in such a way that abstractions SEEM natural.
				(There's a Wittgenstein anecdote along the lines of the following dialogue:
				“Tell me," Wittgenstein asked Phaethon, "why do people always say, it was natural for man to assume that the sun went round the earth rather than that the earth was rotating?" Phaethon replied, "Well, obviously because it just looks as though the Sun is going round the Earth." Wittgenstein replied, "Well, what would it have looked like if it had looked as though the Earth was rotating?”...we could generalize this intuition, which I think is at the heart of Wittgenstein to: What would it look like ``And what would it look they if they looked as if they were not natural kinds?'')

			McClelland gives three great examples in http://psych.stanford.edu/~jlm/papers/McClelland15HbkLangEmergence_Proof.pdf
			Quasiregularity: how THIS is about natural kinds

			A clever thought that might occur at this point is that there aren't any natural kinds at all. This is just a rephrasing of Quine's indeterminacy of translation thesis, which is obvious, but people don't really seem to point it out explicitly.


			For instance, the Twin Worlds argument is completely analogous to a justification of phonemes being abstract units:



			4: So the obvious question arises: if there are no natural kinds, how on earth DOES cognition acheive flexibility and systematicity?

			This is where connectionism enters the picture. Neural nets are actually pretty good at being systematic

			You could think of cognition as a compositionality simulator. It takes complicated functions and attempts to break them down into simpler functions which


			4: people often say that the problem with machine learning is that we don't understand why it works. But that's the whole point. What people want is something compositional. For instance, if you said, ah, vision works by finding lines then etcetc, you've abstracted away from the detailed. Explanation isn't the act of finding something underlying, it's the act of imposing a model. (incidentally, this intuition is, I think, completely analogous to ideas from a completely and utterly different area, namely literary criticism: pretty much what that Wesley Phoas paper is about).

			5: so we have philosophical scepticism over natural kinds, and we have machine learning approaches to AI which appear to succeed by not assuming natural kinds. Putting 2 + 2 together, we might wonder whether this is precisely why they succeed...

			symbolic representations of data (i.e. abstractions) in GOFAI are natural kinds. For example, a [b]

			6: my anticipated responses are: I don't get what the big deal is. I
			a number of confusions that might arise at this point:
				but language is clearly compositional!
				what does this change matter anyway?




			6: types:

				Spelling things out with types is helpful. Let's call an object which is a  an Abs(a), that is: it's an
					e.g. the meaning of a sentence: you can't access the thing of type a directly, but you can use it in a further computation. This is precisely what monads model. You have a type (M a) and you can't unwrap the type constructor M. However, you can pass (M a) into a function (a -> M a) in such a way that you're using the information in the a.

					Note the key consequence of this: anything you get out is also wrapped, so can't be inspected, but just used. This is key.

					So a distributed representation of a symbol can be thought of as a value wrapped in a type constructor Abs.
					Alternative: an abstract symbol (natural, NOT conceptual) can be thought of as an a.
						so thinking that abstraction is trivial is like pulling an a out of an Abs(a)

					POINTER
				confusion: wait, I think that free will and det are of same type and I'm trying to collapse dichot. But if they are compatible, surely i'd want to argue that they're of different types?


			When confronted with distributed representations, it is natural to want to extract the symbolic knowledge from the system. This is a category error. The most obvious mistake would be to try to correlate individual notes to
				this overeager homomorphism

			But even though this is clearly misguided, people commonly backoff to a less obviously mistaken view, that a set or even the whole net encodes precisely what the symbol is.

			So in summary, GOFAI and compositional semantics, and a whole set of essentialist philosophical stances are motivated, at their heart, by a notion that certain objects are natural kinds. And by denying the primacy of natural kinds

			Of course they appear natural

			This view in terms of natural kinds pinpoints exactly what is at stake
