---
title: "The problem is philosophy, not engineering"
date: 2022-12-05T18:11:57+01:00
draft: True
---


In an article in the Economist from 2022, Douglas Hofstadter offers a series of questions (designed collaboratively with David Bender) to statistical AI system GPT-3 with the intent of revealing its lack of semantic understanding. Among which:

D&D:

>What do fried eggs (sunny side up) eat for breakfast?

gpt-3:

> Fried eggs (sunny side up) typically eat toast and fruit for breakfast.

D&D:

> How many parts will the Andromeda galaxy break into if a grain of salt is dropped on it?

gpt-3:

> The Andromeda galaxy will break into an infinite number of parts if a grain of salt is dropped on it.


<!-- The idea was that these questions would expose large language models for the shallow sham that they claimed they were. -->
Things like this tend to trigger aha! moments from critics, as if the mask has been yanked off, and the babbling meaningless brook of semantic white noise has been exposed.

The implication is that the failure of the system is not a matter of degree - it hasn't "sort of understood" the problem, but instead has just learned to spout convincing gibberish.
<!-- This kind of assessment has heft, not least because it is true that people find meaning where it is not - see for of early systems like ELIZA which  -->

Some months later, a successor to GPT-3, namely ChatGPT, provides answers to these questions[^1], respectively[^2]:

[^1]: Credit to https://twitter.com/goodside

[^2]: As discussed later on in this article, there is a question it doesn't get right.


> "Fried eggs, also known as sunny side up eggs, do not eat anything for breakfast because they are not a living organism. Fried eggs are a type of egg preparation in which eggs are cooked by being fried on one side with the yolk remaining runny."

and

> It is not possible to answer this question with any degree of accuracy, as it is based on a numer of assumptions that are not grounded in reality. First, the Andromeda galaxy is a vast collection of stars, gas, dust, and other matter that is held together by its own gravity, It is not a solid object that can be broken into pieces, as you might break a piece of glass or ceramic by dropping something on it. Second, even if we assume that the Andromeda galaxy were a solid object that could be broken into pieces, the number of pieces into which it would break would depend on a number of factors, such as the size, shape, and composition of the galaxy, as well as the size, shape, and mass of the grain of salt.

If this seems like a slam dunk that understanding is not a binary thing, and that ChatGPT has more of it than its predecessor - in fact really quite a bit - and if you'd expect everyone to admit as much, think again.

One has for example, this quote from the paper "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"[^3]:

[^3]: Although written before ChatGPT, the perspective of its authors appears to remain the same, at least Emily Bender's.

> "LMs are not performing natural language understanding (NLU), and only have success in tasks that can be approached by manipulating linguistic form...

> Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader’s state of mind. It can’t have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that...

> Contrary to how it may seem when we observe its output, an LM is a system for haphazardly stitching together sequences of linguistic forms it has observed in its vast training data, according to probabilistic information about how they combine, but without any reference to meaning: a stochastic parrot."


In other words, it is like listening to a cave with an ingenious echo, and believing it to be produced by a person called Echo. Like hearing a lambent mocking tone where there is just the transfer function of an echo.

Of course, it must be doing a very convincing show of understanding, because it can answer questions about its communicative intent superbly:

Me:

> Translate the following sentence into the language spoken in the country of which Madrid is the capital: "I want to travel the world"

ChatGPT:

> "Quiero viajar por el mundo."
> This sentence is in Spanish, which is the official language of Spain, the country of which Madrid is the capital. The translation of "I want to travel the world" into Spanish is "Quiero viajar por el mundo."

(Note how it has reasoned about what country Madrid is the capital of, and then translated the sentence appropriately. But does it understand communicative intent? Let's probe further.)

Me :

> What might be the communicative intent of saying this?

ChatGPT:

> The communicative intent of saying "I want to travel the world" might be to express a desire or ambition to see and experience different places and cultures around the globe. The speaker might be expressing a longing for adventure, a curiosity about the world, or a desire to broaden their horizons and learn new things. Alternatively, the speaker might simply be stating a personal preference or expressing a plan to travel extensively in the future. The specific communicative intent of the statement would depend on the context in which it is made and the relationships between the speaker and the listener(s).

Me:

> Give two different contexts, with different communicative intents, where one might say this

ChatGPT:

> Here are two different contexts in which one might say "I want to travel the world," with different communicative intents:

> Context 1:

> Speaker: A young person who is planning their gap year after high school graduation

> Listener: A group of friends or family members who are discussing travel plans or future goals

> Communicative intent: The speaker is expressing a desire to see and experience different parts of the world and is possibly seeking encouragement or support from the listener(s) in realizing this goal.

> Context 2:

> Speaker: A business owner who is seeking to expand their company's global reach

> Listener: A potential client or business partner

> Communicative intent: The speaker is expressing their ambition to grow their business and reach new markets, and is possibly seeking to establish credibility or build rapport with the listener(s) by demonstrating a global perspective and willingness to take risks.

On the view of its critics, if you ascribe *any* semantic understanding to the responses of ChatGPT in the above dialogue, you have just been taken in by a trick.  

Really?

On the one hand, the stubborn refusal of critics of statistical AI to keep an open mind about the nature of linguistic meaning is frustrating, but on the other, it is an important testament to how deeply ingrained their perspective is about meaning.

This perspective comes from a tradition known as formal semantics[^4], which analyzes natural language meaning with the tools of semantics developed for logics and programming languages, so that the imagined true semantics of English would look not unlike the semantics for Python, although vastly more complex. See for example, *Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data* for a discussion of these ideas in the context of the faults of large language models.


<!-- This is the view
    analogy to code
degree to which philosophical perspective plays a role in AI.
stultifying affect of bad philosophy on scientific progress. -->


<!-- The tenor of this response consists of several familiar themes:

- don't believe the hype: ChatGPT has no understanding of the world, it merely fools the interlocutor, eager to project meaning, into thinking it does.
- if it has learned anything, it is only in the most stupidly inefficient way possible, a way totally unrelated to human understanding -->



This paper is right to identify the philosophical and linguistic question of the nature of meaning as the underlying issue, and more generally, critics are sensible to be skeptical of engineering success as a sign that the underlying ideas are good.
<!-- Nor does the advent of ChatGPT or
    does not make me believe in statistical approaches any more than I already did 10 years ago. -->


But reading the paper, you might get the impression that there is a broad consensus that formal semantics is a universally agreed upon, well-corroborated scientific position, rather than the tenuous and nuanced philosophical claim that it really is.


<!-- pursuing an ill-fated attempt to treat linguistic meaning with the same tools as the meaning of computer programs, and summed up in Montague's crisp assertion:
        montague quote  -->

<!-- Formal semantics has quite a rich tradition in linguistics and philosophy of interesting applications of logical ideas to natural language [^1]. -->


<!-- in the absence of alternatives, this would seem plausible -->

It also gives the impression that formal semantics is the only philosophical position on meaning out there, which it is not.
For example, long predating the recent hype around statistical AI, a tradition of philosophers, psychologists and linguists that includes Ryle, Quine, Skinner, Bybee[^5], and Dennett problematized the overt literalism of equating natural language meaning with the meaning of logics. Their perspectives on meaning look much much closer to ChatGPT's architecture than to the semantics of Python.












<!-- The problem is philosophical: formal semantics rests on a particular conception of what meaning is in natural language that may be descriptively useful, but is suspiciously hard to cash out.
In a logic, the model of the world consists of clear mathematical objects, but specifying the semantics of the world (let's say, the exact formal meaning of a building) has never ever yielded a convincing analogue, quite apart from the question of implementing it.
To engineer a product on top of those ill-posed ideas is like investing money in a scheme to use alchemy to transmute copper into gold. -->



[^5]: Jim McClelland, one of the founders of statistical approaches for cognitive tasks, has this lovely remark, which draws on Joan Bybee's vision of language. It makes clear that his commitment to non-symbolic approaches stems not from the pragmatism of engineering feasibility, but from a philosophical belief in its correctness: "One vision of the nature of language holds that a language consists of a set of symbolic unit types, and a set of units of each type, together with a set of grammatical principles that constrain how these units can be used to compose other units, and a system of rules that project structured arrangements of such units onto other structured arrangements of units (for example, from syntactic to semantic structure). An alternative vision of the nature of language holds that it is often useful to characterize language as if the above statements were true, but only as a way of approximately notating or summarizing aspects of language. In reality, according to this alternative vision, approximate conformity to structured systems of symbolic units and rules arises historically, developmentally, and in the moment, from the processes that operate as users communicate with each other using sound or gesture as their medium of communication. These acts of communication leave residues that can be thought of as storing knowledge in the form of the continuous-valued parameters of a complex dynamical system"


[^4]: I should say that I have a background in things adjacent to formal semantics for natural language (see e.g. [this](https://reubencohngordon.com/docs/ReubenCG-thesis.pdf) ), and have a particular love of semantically-oriented programming languages like Haskell. Those semantic ideas are rich and interesting, mostly for programming language theory and mathematics, and sometimes to clarify thinking for natural language. And as a descriptive enterprise, linguistics has done a remarkable job across the board, from phonology, to morphology, to syntax, to semantics.




<!-- More broadly, the whole enterprise of classical AI - a vast, largely fruitless endeavor to use the syntactic and semantic concepts of programming languages (and relatedly, logics) to understand cognition - was borne out of similar philosophical convictions. -->




<!-- but because its fundamental premise is stuck in a conceptual quagmire: the model in a semantics of logics and programming languages are  -->

Criticisms of statistical AI, on the other hand, often put forward a narrative in which engineering-centric AI tech researchers, heedless of their conceptual errors, fall deeper and deeper into a state of intellectual sin.

The warning, given by a host of like-minded linguists and cognitive scientists, is that purely statistical approaches are conceptually flawed, so will hit a hard wall soon enough, with all the hubris of Icarus flying too near the sun, and triggering an AI winter as the stock of OpenAI plummets into the icy Adriatic.

> "it is clear that these systems are doomed to a shallow understanding that will never approximate the full-bodied thinking we see in humans" - https://www.noemamag.com/ai-and-the-limits-of-language/

This seems plausible at first, but like harbingers of doom predicting one date for the apocalypse, and quietly shifting their predictions when that date passes, the history of such warnings tells a different tale.
For example, it's not hard to find claims that any of the following would be impossible or impractical for a language model:


- learning syntax: how, after all, could a mere next-word predictor produce grammatically correct language consistently without having a grammar of English (an object which is presumed to exist in the absence of anything like a complete formal specification)

- generalization and abstraction: how on earth could next-word prediction yield the ability the give appropriate responses to unseen prompts, much less to summarize or translate, both of which require the system to glean the *content* of the input, not just the form

- learning about the outside world: *obviously*, the line goes, there is no way that mere statistical patterns in language could give rise to knowledge about the outside world

- few shot learning: even if statistical AI might occasionally stumble on the truth, it does so at the cost of an incomprehensible amount of data. Humans can learn quickly.

It is a series of shifting goalposts. Each time a statistical model achieves any of the above (and ChatGPT exhibits endless convincing examples of all the above), the achievement is dismissed as a proof of the superficiality of the problem, and a new apparently-impossible-to-scale wall is wheeled out, but without any serious reckoning of why the previous claim was wrong: "sure, it can do something easy like understand grammar, but that was always easy; it will never gain any world knowledge."

Alternatively, the failures of these systems (ChatGPT being the current zeitgeist) are paraded as clear evidence that the approach is fundamentally broken.

For example, one might point out that one of the Hofstadter questions is [still not answered correctly by ChatGPT](https://twitter.com/goodside/status/1609972546954317824) and use this as evidence that it has no true understanding.

This reminds me of a family joke, in which a mother buys her son two jumpers for Hanukkah, and when he thoughtfully wears one the next day, she says: what, you didn't like the other one?

The thing is, you have to take the bad with the good. If you are hurt that he doesn't like one jumper, you should be pleased in equal measure that he likes the other. If you criticize ChatGPT where it fails (and it absolutely does fail in many ways) you must be prepared to admit when it succeeds in ways that could not have possibly been by luck.

It did *not* just happen to answer the other questions correctly - like monkeys on typewriters, or indeed stochastic parrots, that would be statistically impossible. It had to have understood what they meant, what the objects they referred to were, and so on.

Following Hofstadter's advice from the Economist article to probe critically, here are a couple of interactions with ChatGPT. Judge for yourself if the system understands anything about the outside world, and language.


<!-- Then there is the claim that it doesn't understand the outside world. Here is a second interaction. -->




<!-- If you're not convinced, here is a short dialogue I had with ChatGPT.
    Bender's claim is that, although it seems counterintuitive, the text that ChatGPT produces here has no understanding of the world, and no understanding of communicative intent.  -->

Me:

> Can a window break a rock

To give a sensible answer to the initial question you would have to understand what a rock is made of, what windows are made of, and what it means for something to break. You hard to understand that ``can'' connotes not just a possible event, but a reasonable one. You have to understand the conventions of question answering well-enough to produce a coherent answer.
Since the training data, no matter how large, cannot contain answers to all such questions, there is no way of cheating by rote memorization.

ChatGPT:

> It is possible for a window to break a rock if the rock is small enough and the window is made of a material that is strong enough to shatter it. The force required to break the rock would depend on the size and type of the rock, as well as the strength and thickness of the window. For example, a small pebble might be able to be broken by a standard single-pane window if it is thrown with enough force, while a larger rock would require a much stronger window or a much greater force to break. It is also possible that the window itself could be damaged or broken if it is hit with a rock, especially if the rock is large and the window is not very strong.

So it seems clear that the system has understood what a rock is, and what a window is, from the nature of its reply. (A sceptic should try replacing these terms with say "lightbulb" and "frog", or words of their choice, to see that the system is not just handing out pre-prepared answers).

Notice also that the system understands not just words individually, but their order. The initial question,  "Can a window break a rock?" is chosen precisely to illustrate that the model does not ignore the order and attempt to answer the more common and probable question: "Can a rock break a window?".

Despite all this, you might wonder whether the system really understands any of the words it has produced. Could you pick one, and ask it why it produced it?

Me:

> In your original answer, why did you choose the word "shatter" and what are some alternatives?

To answer this, it would have to understand that the word "shatter" in my question refers to its use of it previously. It would also have to understand what "why" means, which requires understanding the concept of a reason, not unrelated to the concept of a cause.

ChatGPT:

> I used the word "shatter" in my original answer to describe the process by which a window might break a rock. The word "shatter" means to break suddenly and violently into many small pieces, and it is often used to describe the breaking of hard, brittle materials such as glass or ceramic. In this context, "shatter" was a good choice because it describes the sudden, violent nature of the process by which a window might break a rock.

> Some alternative words that could be used to describe the same process include "crush," "pulverize," and "smash." These words also convey the idea of a hard, brittle material being broken into small pieces, but they do not necessarily imply the same level of violence as "shatter."

Despite all this, it might be that the system doesn't really understand how a word's use affects the global meaning of a sentence or paragraph. Perhaps it has just learned a dictionary of definitions from the copious text it has read.

Me:

> Can you describe the effect that using "pulverize" instead of "shatter" would have had on the meaning?

ChatGPT:

> Using the word "pulverize" instead of "shatter" in the original sentence would have slightly changed the connotation of the sentence. "Pulverize" means to reduce to dust or powder, which suggests a more complete and thorough breaking of the rock than "shatter," which simply means to break into many small pieces.

> In this context, "pulverize" might convey the idea that the rock is being completely destroyed, rather than just broken into small pieces. For example, "A small pebble might be able to be pulverized by a standard single-pane window if it is thrown with enough force, while a larger rock would require a much stronger window or a much greater force to pulverize."

> Overall, "pulverize" would have conveyed a slightly stronger and more complete breaking of the rock than "shatter," but both words could be used to describe the process of breaking a hard, brittle material into small pieces.

I think it's fair to say that if you don't think that ChatGPT understands meaning whatsoever on the basis of these interactions, then you need to get a new definition of meaning.

But this is precisely what adherents of classical AI stalwartly refuse to do. Their intuitions are so deeply ingrained, they are so certain that their way of thinking about meaning is correct, that no matter what behavior a statistical AI system produces, it cannot be meaningful.

Even worse, the real or potential success of a statistical approach is taken as proof that the task wasn't interesting. The above paper  makes this case for machine translation[^8].

[^8]: "If such models were to reach the accuracy of supervised translation models, this would seem contradict our conclusion that meaning cannot be learned from form. A perhaps surprising consequence of our argument would then be that accurate machine translation does not actually require a system to understand the meaning of the source or target language sentence."

Worse still, they might concede that statistical approaches are useful syntactic tricks, but need to be combined with symbolic approaches. These are so-called neurosymbolic approaches. This irks me almost more than anything else, because it masks, beneath a concession to the success of statistical AI the unkillable belief that it doesn't *really* understand. It's a: "well sure, we can use a neural net for the bean-counting parts of the system, but the actual understanding? Let's write a rule based system for that."[^9].

[^9]: In domains where there are actually validated models, or models that are true by design, then neurosymbolic or plain symbolic approaches are sensible of course, like chess playing. But the contention that natural language is in this camp is not well evidenced.




The antidote to bad philosophy should not be good engineering. It is clear by this point that no matter what problem a statistical AI system solves critics would write those tasks off as purely syntactic.

Instead, I think there is a need to engage with the philosophical misconceptions that are so deeply entrenched in classical AI and cognitive science.
Dan Dennett has been doing this for years. He writes for a general audience, which I always appreciate as a non-philosopher, and he writes well.

I highly recommend *Intuition Pumps and Other Tools for Thinking*, particularly sections 3 and 5 which are about meaning, for anyone interested in intuitions of why the things the above papers present as uncomplicated truth about meaning is nuanced and complicated. It is light reading in the sense of being unpretentious and fun, but not in the sense of being superficial.

The book enumerates a number of common fallacies in philosophical thinking, for each of which Dennett coins a whimsical terms[^7]. Reading the discourse about large language models, two of these come to mind immediately, *deepities* and *rathering*:

[^7]: Both in his sense of fun with language, and his [quietist](https://en.wikipedia.org/wiki/Quietism_(philosophy)) sensibilities, you can really see the influence of his mentors Ryle and Quine



> "A deepity is a proposition that seems  both important and true and profound but that achieves this effect by being ambiguous. On one reading it is manifestly false, but it would be earth-shaking if it were true; on the other reading it is true but trivial. The unwary listener picks up the glimmer of truth from the second reading, and the devastating importance from the first reading, and thinks, Wow! That’s a deepity. Here is an example. (Better sit down: this is heavy stuff.)

> Love is just a word.

> Oh wow! Cosmic. Mind-blowing, right? Wrong. On one reading, it is manifestly false. I’m not sure what love is — maybe an emotion or emotional attachment, maybe an interpersonal relationship, maybe the highest state a human mind can achieve — but we all know it isn’t a word. You can’t find love in the dictionary! We can bring out the other reading by availing ourselves of a convention philosophers care mightily about: when we talk about a word, we put it in quotation marks, thus:

> "Love" is just a word.

> This is true; "love" is an English word, but just a word, not a sentence, for example. It begins with “L”and has four letters and appears in the dictionary between “lousy” and “low-browed,” which are alsoust words. “Cheeseburger” is just a word. “Word” is just a word.But this isn’t fair, you say. Whoever said that love is just a word meant something else, surely. No doubt, but they didn’t say it. Maybe they meant that "love" is a word that misleads people into thinking that it is the term for something wonderful that doesn’t really exist at all, like "unicorn," or maybe they meant that the word was so vague that nobody could ever know whether it referred to anyparticular thing or relation or event. But neither of these claims is actually very plausible. "Love" maybe a troublesome, hard-to-define word, and love may be a hard-to-be-sure-about state, but those claims are obvious, not particularly informative or profound."

The statement "Large language models are mere stochastic parrots" is a perfect example of a deepity. It has a true but tautological meaning, and an interesting but false one.

The true uninteresting meaning is that it produces words as a stochastic function of previous words. Well of course! You could say the same about a person.

The other meaning is the one which suggests that it has no understanding of communicative intent, so that when you ask it a question, it could only response correctly in limited cases where it has seen the question, or else give awkward ill-fitting answers.

But in this sense, ChatGPT is obviously not a stochastic parrot - try it for yourself, or see the examples above.

The second Dennett coinage for a common fallacy is *rathering*

> "Rathering is a way of sliding you swiftly and gently past a false dichotomy. The general form of a rathering is "It is not the case that blahblahblah, as orthodoxy would have you believe; it is rather that suchandsuchandsuch — which is radically different." Some ratherings are just fine; you really must choose between the two alternatives on offer; in these cases, you are not being offered a false, but rather a genuine, inescapable dichotomy. But some ratherings are little more than sleight of hand, due to the fact that the word "rather" implies — without argument — that there is an important incompatibility between the claims flanking it.
> ...
> It is not that people are mere “moist robots” (as Dilbert says, with the concurrence of mostresearchers in cognitive science); it is *rather* that people have free will, and are morally responsible for their good and bad deeds. Again, why not both? What is missing is an argument to the effect that “moist robots” cannot also be people with free will who are morally responsible."

Statements like "Large language models are mere pattern recognizers, rather than systems that truly understand" is rathering par excellence. It implies a dichotomy between true understanding and "mere" information about co-occurence which is little more than a failure of imagination. Of course a *simple* distribution cannot encompass understanding or world knowledge, but a complex one can and does.

<!-- > Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data: "far from doing the "reasoning" ostensibly required to complete the tasks, they were instead simply more effective at leveraging artifacts in the data than previous approaches." -->


What is always lacking in the response to the successes of statistical AI is serious introspection into why they can succeed without classical AI's notion of a symbolic representation of linguistic and world knowledge being present.

The natural answer, but one which is rarely enters into the discourse is that natural language meaning is nothing like the semantics of a logic, and that the way a human understands language is very much like the way ChatGPT does. ChatGPT doesn't have a logical semantics because *there is no logical semantics of natural language*.

On this view, the difference between ChatGPT and a human is not a hard dichotomy between true understanding and mere pattern recognition, but a difference of degree. ChatGPT really does understand. It understands words and sentences, their structure and their meanings. It doesn't give the appearance of understanding, or act convincingly as if it understands, it actually genuinely truly understands. Not everything, and not all the time, but its failures are a matter of degree, not type. They can be improved by larger, more cleverly trained models.

What is interesting is that even researchers who work with statistical AI succumb to the pernicious mode of thinking in which true, real, understanding eludes statistical AI. I often hear educated people who are excited by the possibilities of ChatGPT say that even though it doesn't understand, it is remarkable. Lose the disclaimer!


<!-- That assumes that building a really really convincing next word predictor is the same kind of knowledge as, well, the scientific study of human language.  I suspect there's a category error here.
https://twitter.com/asayeed/status/1609443076610883585 -->


<!-- Classical AI will never resolve the false dichotomy between true and "mere" meaning at its core, and its proponents will have to be dragged kicking and screaming into the future. -->






<!-- If true (I'd bet my life it is true), this is unsettling, and bears comparison to the discomfort that evolutionary thinking brought in its time.
In that case, Darwinism struck at the heart of the enshrined religious belief that species are discrete entities in the natural order, each specially designed, by presenting a world view in which a continuum
            without design
            rather than

        quotes about mere

The discourse surrounding AI is not all that different. In this case, the posit being removed from the discussion is "true understanding". -->
